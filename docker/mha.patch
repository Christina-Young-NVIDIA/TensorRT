diff --git a/torch/nn/functional.py b/torch/nn/functional.py
index 762990ee93..99bef847c9 100644
--- a/torch/nn/functional.py
+++ b/torch/nn/functional.py
@@ -4975,7 +4975,7 @@ def multi_head_attention_forward(
         f"was expecting embedding dimension of {embed_dim_to_check}, but got {embed_dim}"
     if isinstance(embed_dim, torch.Tensor):
         # embed_dim can be a tensor when JIT tracing
-        head_dim = embed_dim.div(num_heads, rounding_mode='trunc')
+        head_dim = int(embed_dim.div(num_heads, rounding_mode='trunc'))
     else:
         head_dim = embed_dim // num_heads
     assert head_dim * num_heads == embed_dim, f"embed_dim {embed_dim} not divisible by num_heads {num_heads}"
@@ -5044,6 +5044,7 @@ def multi_head_attention_forward(
     #
     # reshape q, k, v for multihead attention and make em batch first
     #
+    bsz = int(bsz)
     q = q.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)
     if static_k is None:
         k = k.contiguous().view(k.shape[0], bsz * num_heads, head_dim).transpose(0, 1)
